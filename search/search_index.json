{
    "docs": [
        {
            "location": "/",
            "text": "Home\n\u00b6\n\n\n\nHi, I'm Aayush Uppal.\n\nA Software Engineer , currently working for \nBloomberg\n in New York City and a \n\nUniversity at Buffalo\n/ \n\nNIT Hamirpur\n alumni.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI am passionate about \nDistributed Systems\n, \nData Science\n, \nMachine Learning\n and in process building high performance\n, reliable backend systems to leverage those capabilities to the maximum and derive the best insight possible.\n\n\n\n\nCurrent Endeavors\n\u00b6\n\n\n\n\nBuilding ETL pipelines with Apache Spark.\n\n\nReal Time Analytics using Spark Streaming and Spark MLlib.\n\n\nUnderstanding Hyperledger and Blockchain.\n\n\nDwelling into the fullstack Javascript world with MEAN stack.\n\n\n\n\nFeatured\n\u00b6\n\n\n\n\n\n\nKangra Valley Train in the stunning backdrop of Dhauladhars. \nlink\n\n\n\n\n\n\nMore About Me\n\u00b6\n\n\n\n\n\n\n\n\nHome is where the Himalayas reside\n\n\n\n\nI grew up in a picturesque town on the edge of Himalayas. \nDharamshala\n\n\nCheck out my thread dedicated to the amazing place. \nLink\n\n\n\n\n\n\n\n\nAmongst other things: A passion for Football and the Blues Guitar, usually gets me through some of my days.",
            "title": "Home"
        },
        {
            "location": "/#home",
            "text": "Hi, I'm Aayush Uppal. \nA Software Engineer , currently working for  Bloomberg  in New York City and a  University at Buffalo /  NIT Hamirpur  alumni.       I am passionate about  Distributed Systems ,  Data Science ,  Machine Learning  and in process building high performance\n, reliable backend systems to leverage those capabilities to the maximum and derive the best insight possible.",
            "title": "Home"
        },
        {
            "location": "/#current-endeavors",
            "text": "Building ETL pipelines with Apache Spark.  Real Time Analytics using Spark Streaming and Spark MLlib.  Understanding Hyperledger and Blockchain.  Dwelling into the fullstack Javascript world with MEAN stack.",
            "title": "Current Endeavors"
        },
        {
            "location": "/#featured",
            "text": "Kangra Valley Train in the stunning backdrop of Dhauladhars.  link",
            "title": "Featured"
        },
        {
            "location": "/#more-about-me",
            "text": "Home is where the Himalayas reside   I grew up in a picturesque town on the edge of Himalayas.  Dharamshala  Check out my thread dedicated to the amazing place.  Link     Amongst other things: A passion for Football and the Blues Guitar, usually gets me through some of my days.",
            "title": "More About Me"
        },
        {
            "location": "/profile/",
            "text": "Profile\n\u00b6\n\n\nWork\n\u00b6\n\n\nBloomberg - Software Engineer\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\nNew York, NY : May 2016 - Present\n\n\n\n\nWorking as Software Engineer in Internal Systems and Web Applications group.\n\n\nHighlights\n\n\nApache Spark based lifecycle analysis\n\n\nCommunication Systems Distributed backend systems, core feature and infrastructure\n\n\nTools for continuous integration and development, a generic oozie based hadoop job manager\n\n\n\n\n\n\nKey Aspects: Design and Development, C++, Python, Distributed Systems, Data Science, Apache Spark\n\n\n\n\n\n\nAmazon - Software Development Engineer, Intern\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\nSeattle, WA : May 2015 - August 2015\n\n\n\n\nWorked on designing a Pipeline based on TCP Anycast for experimental requests, reduced IP space consumption and performance analysis.\n\n\nConfigured Anycast on a massive distributed network. Setup a pipeline to send requests via TCP Anycast to CDN cache servers.\n\n\nCollected RTT readings and setup Kinesis pipeline for processing.\n\n\nWrote a Package for automated daily processing of day\u2019s RTT data and to generate comparative analysis reports between TCP Anycast and Latency Routing.\n\n\nKey aspects: Object oriented design, multi-threaded programming, Big Data, Cloud Computing.\n\n\nSkills: Linux shell scripting, Perl, Java, Python, Kinesis, Elastic Map Reduce, Hive\n\n\n\n\n\n\nSubBoard Inc | University at Buffalo - Student Assistantship, Web Developer\n\u00b6\n\n\n\n\nBuffalo, NY : Oct 2014 - Dec 2015\n\n\n\n\nWorked as Web Admin anfd developer for SubBoard Web Portal.\n\n\nKey aspects: complete ownership - architecture, design, and maintainence of systems. Skills: MySQL, Server Administration, Amazon Web Services, Wordpress\n\n\n\n\n\n\nCompro Technologies - Software Engineer\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\nNew Delhi, India : August 2013 - July 2014\n\n\n\n\nMyITLab Sims project\n\n\nCloud based HTML5 component development of MS Excel, MS PowerPoint features. Developed key components for web module including drawing pane, grid functionalities and graph components.\n\n\nThis product is an industry leading training and assessment tool for professional and higher education segments.\n\n\nKey aspects: Agile development, testing, web services, object oriented analysis and design.\n\n\nSkills: Java, JavaScript, jQuery, SQL, HTML5, CSS\n\n\n\n\n\n\n\n\n\n\nNational University of Singapore - Researcher, Intern\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\nSingapore : May 2012 - July 2012\n\n\n\n\nSummer Intern at Solar Energy Research Institute of Singapore @ NUS, Singapore\n\n\nWorked on Raytracing of Solar Cells, to comeup with an effective implementation for simulating\nlight trapping effeciency in solar cell models, involved mathematical, algorithm modeling\n\n\nRelied on C++ for extensibilty and better performance for the implementation\n\n\nRaytracing for Solar Cells\n\n\n\n\n\n\nEducation\n\u00b6\n\n\nState University of New York, University at Buffalo\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\nBuffalo, New York : August 2014 - February 2016\n\n\n\n\nMasters in Computer Science\n\n\nGPA \n3.86/4\n\n\nHighlights\n\n\nCoursework focus: Distributed Systems, Artificial Intelligence and Machine Learning\n\n\nProjects\n\n\nSemantic Labeling of Images using SVM and Markov Blanket\n\n\nAndroid Group Messenger\n\n\nClassification & Diabetes Level Prediction\n\n\nMood based songs playlist using EEG Signals\n\n\nNeural Network for Classification of Handwritten Digits\n\n\nDynamo style key value storage implementation\n\n\nBayes Localization for Robot Motion\n\n\nImage Processing: Photometric Stereo Toolbox Implementation\n\n\nDisparity & Depth estimation of Stereo view images using Dynamic Programming\n\n\nBoolean Query Search engine on news corpus\n\n\nAlumniConnect\n\n\nDistributed Hash Table\n\n\nNews Search Engine, Story Representation and Analytics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNIT Hamirpur\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\nHamirpur, India : August 2009 - May 2013\n\n\n\n\nBachelors of Technology, Electronics and Communication Engineering\n\n\nGPA \n7.5/10\n\n\nHighlights\n\n\nDirectorate of Technical Education Scholarship Holder\n\n\nCo-convener INS & Controls\n\n\nTeam leader, Electronics and Communication Engineering\n\n\nExecutive Student Member, \nTIFAC Core Project\n\n\nDam Warning System using GSM\n\n\nPerformance Evaluation of WCDMA system",
            "title": "Profile"
        },
        {
            "location": "/profile/#profile",
            "text": "",
            "title": "Profile"
        },
        {
            "location": "/profile/#work",
            "text": "",
            "title": "Work"
        },
        {
            "location": "/profile/#bloomberg-software-engineer",
            "text": "New York, NY : May 2016 - Present   Working as Software Engineer in Internal Systems and Web Applications group.  Highlights  Apache Spark based lifecycle analysis  Communication Systems Distributed backend systems, core feature and infrastructure  Tools for continuous integration and development, a generic oozie based hadoop job manager    Key Aspects: Design and Development, C++, Python, Distributed Systems, Data Science, Apache Spark",
            "title": "Bloomberg - Software Engineer"
        },
        {
            "location": "/profile/#amazon-software-development-engineer-intern",
            "text": "Seattle, WA : May 2015 - August 2015   Worked on designing a Pipeline based on TCP Anycast for experimental requests, reduced IP space consumption and performance analysis.  Configured Anycast on a massive distributed network. Setup a pipeline to send requests via TCP Anycast to CDN cache servers.  Collected RTT readings and setup Kinesis pipeline for processing.  Wrote a Package for automated daily processing of day\u2019s RTT data and to generate comparative analysis reports between TCP Anycast and Latency Routing.  Key aspects: Object oriented design, multi-threaded programming, Big Data, Cloud Computing.  Skills: Linux shell scripting, Perl, Java, Python, Kinesis, Elastic Map Reduce, Hive",
            "title": "Amazon - Software Development Engineer, Intern"
        },
        {
            "location": "/profile/#subboard-inc-university-at-buffalo-student-assistantship-web-developer",
            "text": "Buffalo, NY : Oct 2014 - Dec 2015   Worked as Web Admin anfd developer for SubBoard Web Portal.  Key aspects: complete ownership - architecture, design, and maintainence of systems. Skills: MySQL, Server Administration, Amazon Web Services, Wordpress",
            "title": "SubBoard Inc | University at Buffalo - Student Assistantship, Web Developer"
        },
        {
            "location": "/profile/#compro-technologies-software-engineer",
            "text": "New Delhi, India : August 2013 - July 2014   MyITLab Sims project  Cloud based HTML5 component development of MS Excel, MS PowerPoint features. Developed key components for web module including drawing pane, grid functionalities and graph components.  This product is an industry leading training and assessment tool for professional and higher education segments.  Key aspects: Agile development, testing, web services, object oriented analysis and design.  Skills: Java, JavaScript, jQuery, SQL, HTML5, CSS",
            "title": "Compro Technologies - Software Engineer"
        },
        {
            "location": "/profile/#national-university-of-singapore-researcher-intern",
            "text": "Singapore : May 2012 - July 2012   Summer Intern at Solar Energy Research Institute of Singapore @ NUS, Singapore  Worked on Raytracing of Solar Cells, to comeup with an effective implementation for simulating\nlight trapping effeciency in solar cell models, involved mathematical, algorithm modeling  Relied on C++ for extensibilty and better performance for the implementation  Raytracing for Solar Cells",
            "title": "National University of Singapore - Researcher, Intern"
        },
        {
            "location": "/profile/#education",
            "text": "",
            "title": "Education"
        },
        {
            "location": "/profile/#state-university-of-new-york-university-at-buffalo",
            "text": "Buffalo, New York : August 2014 - February 2016   Masters in Computer Science  GPA  3.86/4  Highlights  Coursework focus: Distributed Systems, Artificial Intelligence and Machine Learning  Projects  Semantic Labeling of Images using SVM and Markov Blanket  Android Group Messenger  Classification & Diabetes Level Prediction  Mood based songs playlist using EEG Signals  Neural Network for Classification of Handwritten Digits  Dynamo style key value storage implementation  Bayes Localization for Robot Motion  Image Processing: Photometric Stereo Toolbox Implementation  Disparity & Depth estimation of Stereo view images using Dynamic Programming  Boolean Query Search engine on news corpus  AlumniConnect  Distributed Hash Table  News Search Engine, Story Representation and Analytics",
            "title": "State University of New York, University at Buffalo"
        },
        {
            "location": "/profile/#nit-hamirpur",
            "text": "Hamirpur, India : August 2009 - May 2013   Bachelors of Technology, Electronics and Communication Engineering  GPA  7.5/10  Highlights  Directorate of Technical Education Scholarship Holder  Co-convener INS & Controls  Team leader, Electronics and Communication Engineering  Executive Student Member,  TIFAC Core Project  Dam Warning System using GSM  Performance Evaluation of WCDMA system",
            "title": "NIT Hamirpur"
        },
        {
            "location": "/devposts/devposts1/",
            "text": "Dev Posts\n\u00b6\n\n\nBuilding with React & NextJS\n\u00b6\n\n\nMarch 2018\n\n\n\n\n//TODO\n\n\n\n\nThe wild west of email threads\n\u00b6\n\n\nFebruary 2018\n\n\n\n\n//TODO\n\n\n\n\n\n\nStreaming analytics on logs\n\u00b6\n\n\n2017\n\n\ndistributed systems\n \nlogging\n \nApache Spark\n \nSpark Streaming\n    \n\n\n\n\nI had been wanting to experiment with the spark streaming framework for a while now, ever since I first got my hands on with the\nspark framework itself to work on a lifecycle analysis application that processed over a TB of data and generated meaningful\nclustered outputs.\n\n\nSpark Streaming for the uninitiated is a powerful platform that enables realtime analysis.\n\n\nThe Problem at hand:\n\nI am sure many developers in this day and age have at some point or the other run into a situation losing\ncontext ever so often while debugging an error or an anomalous behaviour trying to keep track of one identifier for incoming request\nmapped to second and that mapped to the third just to see where in all the event went through and processed in its lifecycle. \nIt particularly becomes even harder when your system is event driven and maintains state for decision making.\n\n\nSo here's what I decided to do. Build a spark streaming application that reads logs from multiple services in realtime from a message queue.\nProcess it in memory up until the corresponding event/request context expires. Aggregate all required information for that particular event \ninto a user defined structured data type. Define trigger conditions for alerts and dumps of error/warn and pass the object containing all the\nlogically mapped info for the program/developer to analyse.\n\n\nThis is what the POC pipeline looked like.\n\n[insert basic pipeline sequence diagram]",
            "title": "DP/Pg/I"
        },
        {
            "location": "/devposts/devposts1/#dev-posts",
            "text": "",
            "title": "Dev Posts"
        },
        {
            "location": "/devposts/devposts1/#building-with-react-nextjs",
            "text": "March 2018   //TODO",
            "title": "Building with React &amp; NextJS"
        },
        {
            "location": "/devposts/devposts1/#the-wild-west-of-email-threads",
            "text": "February 2018   //TODO",
            "title": "The wild west of email threads"
        },
        {
            "location": "/devposts/devposts1/#streaming-analytics-on-logs",
            "text": "2017  distributed systems   logging   Apache Spark   Spark Streaming        I had been wanting to experiment with the spark streaming framework for a while now, ever since I first got my hands on with the\nspark framework itself to work on a lifecycle analysis application that processed over a TB of data and generated meaningful\nclustered outputs.  Spark Streaming for the uninitiated is a powerful platform that enables realtime analysis.  The Problem at hand: \nI am sure many developers in this day and age have at some point or the other run into a situation losing\ncontext ever so often while debugging an error or an anomalous behaviour trying to keep track of one identifier for incoming request\nmapped to second and that mapped to the third just to see where in all the event went through and processed in its lifecycle. \nIt particularly becomes even harder when your system is event driven and maintains state for decision making.  So here's what I decided to do. Build a spark streaming application that reads logs from multiple services in realtime from a message queue.\nProcess it in memory up until the corresponding event/request context expires. Aggregate all required information for that particular event \ninto a user defined structured data type. Define trigger conditions for alerts and dumps of error/warn and pass the object containing all the\nlogically mapped info for the program/developer to analyse.  This is what the POC pipeline looked like. \n[insert basic pipeline sequence diagram]",
            "title": "Streaming analytics on logs"
        }
    ]
}